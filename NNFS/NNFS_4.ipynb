{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Step Activation Function\n",
    "\n",
    "    if weights * inputs + bias >0 THEN neuron will fire and output a 1, otherwise it will output a  0\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linear Activation Function\n",
    "\n",
    "    Equation of a line y=x and the output value equals the input. \n",
    "    Usually applied to the last layer's output in the casae of a regression model(scaler value)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Sigmoid Activation Function\n",
    "\n",
    "    Better than step funtion due the need of optimizers. Sigmoid is more granular and informative. Used in Hidden Layers\n",
    "\n",
    "    y= 1/(1+e^-x)\n",
    "\n",
    "    y=0 and x=-inf,  y=0.5 at x=0,  y= 1 at x=inf\n",
    "\n",
    "    works better with NN due to range between 0 and 1, also keeps all information(Reversible)\n",
    "\n",
    "    Adds non linearity.\n",
    "\n",
    "    Sigmoid eventually gets replaced by Rectified Linear Units (ReLU)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Rectified Linear Activation Function\n",
    "\n",
    "    ReLU is close to being a linear one but remains non-linear due to the bend after 0\n",
    "\n",
    "    y=x when x>0\n",
    "    y=0 when x<0\n",
    "\n",
    "    Simple yet powerful activation funtion is the most widely used due to speed and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
