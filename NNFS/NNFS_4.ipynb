{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Step Activation Function\n",
    "\n",
    "    if weights * inputs + bias >0 THEN neuron will fire and output a 1, otherwise it will output a  0\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linear Activation Function\n",
    "\n",
    "    Equation of a line y=x and the output value equals the input. \n",
    "    Usually applied to the last layer's output in the casae of a regression model(scaler value)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Sigmoid Activation Function\n",
    "\n",
    "    Better than step funtion due the need of optimizers. Sigmoid is more granular and informative. Used in Hidden Layers\n",
    "\n",
    "    y= 1/(1+e^-x)\n",
    "\n",
    "    y=0 and x=-inf,  y=0.5 at x=0,  y= 1 at x=inf\n",
    "\n",
    "    works better with NN due to range between 0 and 1, also keeps all information(Reversible)\n",
    "\n",
    "    Adds non linearity.\n",
    "\n",
    "    Sigmoid eventually gets replaced by Rectified Linear Units (ReLU)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Rectified Linear Activation Function\n",
    "\n",
    "    ReLU is close to being a linear one but remains non-linear due to the bend after 0\n",
    "\n",
    "    y=x when x>0\n",
    "    y=0 when x<0\n",
    "\n",
    "    Simple yet powerful activation funtion is the most widely used due to speed and efficiency."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Activation in the Hidden Layers\n",
    "\n",
    "        No matter what we do with the neurons' weights and biases, the outputs will always be perfectly linear to y=x. This linear nature will continue throughout the entire network\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Activation in a Pair of Neurons\n",
    "\n",
    "        We can see that by using ReLU activation, with different sets of weights(-/+) the slope can change and biases(left/right shift) cause horizontal movement. With the addition of another neuron, now we have 2...the bias of the second neuron shifted the function Vertically. With the 2nd nuerons weight to a -, we now have a neuron that has an activation and deactivation point, whne both neurons are active, they produce values in the range of the granulae, variable, and output. If any neuron is inactive, the pair will produce non-variable output.\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Activation in the Hidden Layers\n",
    "\n",
    "We start with a neural network with 2 hidden layers of 8 neurons each and the layers are NOT densly connected (each N from 1rst connects omly to one N in the 2nd). The model takes 1 value as an input and 1 value as output, just like the sin curve. The output layer uses liner activation, and the hidden layers use rectified linear actvation function. We will now hand tune the Network:\n",
    "        \n",
    "        -Starting with the first/top pair. setting all (3) weights to 1 and all (2) biases to zero, the slope of the overall funtion is simply y=x\n",
    "\n",
    "        - Changing W1 to 6, the slopehas become very steep and now matches the first part of the sin curve. But this function never ends because the neuron pair never deactivates \n",
    "\n",
    "        - By changing B2 to 0.7, this offsets the overall function Vertically upwards\n",
    "\n",
    "        - Setting W2 to a negative number -1, this causes a deactivation point to occur horizontally\n",
    "\n",
    "        - The previous ajustment causes the line that was previously aligned with the sin function to flip over the x axis. In order to flip this slope back. We can take the final weight connected to the output neuron W3 and simply flip it to a -1\n",
    "\n",
    "        - Now all we want is to offset this up vertically. In this Hand optimization we are going to use the first 7 pairs of neurons to create the sin wave's shape and the bottom pair to offset everything vertically.\n",
    "\n",
    "        - Setting the bias of the 2nd neuron in the bottom pair to 1 and the W3 to 0.7 we can vertically shift the line.\n",
    "\n",
    "        - Moving on to the next pair of neurons: The 2nd pair of neurons' activation is beginning too soon. We must ajust the funtion horizontally and then increase the slope. This is done by Adding a bias B1 to -0.42 and chaning the W1 to 3.5\n",
    "\n",
    "        - Now we use the same logic used before with the first pair to set the DEACTIVATION point. This is done by switching W2 to a -1 (negative value essentially flips the activation function) and adding a b2 of 0.27 (function is shifted vertically upwards. The positive bias means that the second neuron starts producing a significant output at a lower input value, and this effect is then reflected in the combined output of the neuron pair. )\n",
    "\n",
    "        - Now we can flip this section's function same as we did before by setting the weight to the output neuron from 1 to -1 \n",
    "\n",
    "        - And again , just like the first pair we use the botton pair to offset \n",
    "\n",
    "        -Finally  , for the third pair of neurons , we only begin the activation for the 3rd pair of hidden layer neurons when we wish for the sop to start going down......We ajust where we want these things to start by the horizontal shifts and flips that we do to the W2 , the W2 turning into a negative creates the flip/deactivation point 'the funtion has become a quesiton of when this neuron deactivates\n",
    "\n",
    "\n",
    "Now we keep applying the same logic to the rest of the neuron pairs as we go trhough mapping the sin function. After passing data through the Network do we see how the neuron's areas of effect come into play--- only when bith neurons are activated. When we input 0.08 we see that its the top layer(the very begining of the sin function/close to zero which makes sense)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU Activation Function Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "\n",
    "output = []\n",
    "for i in inputs:\n",
    "    if i > 0:\n",
    "        output.append(i)\n",
    "    else:\n",
    "        output.append(0)\n",
    "        \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "# We can write the code more simply as we just need to take the larger of the 2 values: 0 or the neuron value\n",
    "\n",
    "\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "\n",
    "output = []\n",
    "\n",
    "for i in inputs:\n",
    "    output.append(max(0, i))\n",
    "    \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "# NumPy contains an equivalent -  np.maximum()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "\n",
    "output = np.maximum(0, inputs)\n",
    "print(output)\n",
    "\n",
    "# this method compares each element of the input list(or an array) and returns an object of the same shape filled with new values.\n",
    "# This will be used in our new rectified linear activation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data\n",
    "import numpy as np\n",
    "import nnfs\n",
    "\n",
    "\n",
    "# The init() function sets the random seed of numpy to a constant, and sets the default data type of numpy to a float type \n",
    "# that is more suitable for neural networks and similar works.\n",
    "nnfs.init()\n",
    "\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        # notice the size of the matrix is n_inputs X n_neurons and not the other way around, we do this to avoid having to transpose the \n",
    "        # weights matrix later. \n",
    "        \n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # 1 X n_n_neurons  Array\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # calculate output values from input\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.        ]\n",
      " [0.         0.00011395 0.        ]\n",
      " [0.         0.00031729 0.        ]\n",
      " [0.         0.00052666 0.        ]\n",
      " [0.         0.00071401 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#  Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# This function generates a 2D dataset with a spiral distribution. It returns the feature vectors (X) and the corresponding labels (y).\n",
    "# The samples parameter specifies the number of data points per class (so you'll have 300 points in total, because you have 3 classes), \n",
    "\n",
    "# Create Dense layera with 2 input features and 3 output values\n",
    "dense1 =  Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Forward pass through activation function\n",
    "# Takes in output from previous layer\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Let's see output of the first few samples\n",
    "print(activation1.output[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative values have been clipped(modified to be zero) from the previous layers outputs(checkout the end of NNFS_3 to see outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Softmax Activation Function"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This projects goal is to be a classifier, so we want an activation function meant for classification. While the rectified linear unit is unbounded and not normalized, the softmax activation produces normalized distributions of probabilities for our classes. Those are called confidence scores and they add up to 1. The predicted class is the associated with the output neuron that returned the largest confidence score.\n",
    "\n",
    "Softmax function:\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values:\n",
      "[121.51041751893969, 3.3534846525504487, 10.85906266492961]\n"
     ]
    }
   ],
   "source": [
    "# outputs from a neural network layer\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "# e - mathematical constant, we use E here to match a common coding \n",
    "# style where constants are uppercased\n",
    "E = 2.71828182846 #you can also use math.e\n",
    "\n",
    "# For each value in a vector, calculate the exponential value\n",
    "exp_values = []\n",
    "for output in layer_outputs:\n",
    "    exp_values.append(E ** output)  # ** - power operator in Python\n",
    "\n",
    "\n",
    "print('exponentiated values:')\n",
    "print(exp_values)\n",
    "# important to note - to calculate the probabilities we need non-negative values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized exponentiated values:\n",
      "[0.8952826639573506, 0.024708306782070668, 0.08000902926057876]\n",
      "Sum of normalized values: 1.0\n"
     ]
    }
   ],
   "source": [
    "# normalization = take a given exponentiated value and divide it by the sum of all the exponentiated values\n",
    "# Since each output value normalizes to a fraction of the sum, all the values are now a range of 0 to 1 and add up to 1\n",
    "# The following is adding the sum and normalization to the code:\n",
    "\n",
    "# Now normalize values\n",
    "norm_base = sum(exp_values) # We sum all values\n",
    "norm_values = []\n",
    "for value in exp_values:\n",
    "    norm_values.append(value / norm_base)\n",
    "print('Normalized exponentiated values:')\n",
    "print(norm_values)\n",
    "\n",
    "print('Sum of normalized values:', sum(norm_values))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values:\n",
      "[121.51041752   3.35348465  10.85906266]\n",
      "normalized exponentiated values:\n",
      "[0.89528266 0.02470831 0.08000903]\n",
      "sum of normalized values: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# We can preform the same set of operations with the use of NumPy in the following way:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Values from earlier\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "# For each value in a vector, calculate the exponential value\n",
    "exp_values = np.exp(layer_outputs)  # E ** output\n",
    "print('exponentiated values:')\n",
    "print(exp_values)\n",
    "\n",
    "# Now normalize values\n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "print('normalized exponentiated values:')\n",
    "print(norm_values)\n",
    "print('sum of normalized values:', np.sum(norm_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train batches we need to convert this funtionality to accept layer outputs in batches:\n",
    "\n",
    "# Get unnormalized probabilities\n",
    "exp_values = np.exp(inputs)\n",
    "\n",
    "# Normalize them for each sample\n",
    "probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "# in a 2d array/matrix, axis 0 refers to the rows, and axis 1 refers to the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum without axis:\n",
      "18.621999999999996\n",
      "This will be identical to the above since default is None:\n",
      "18.621999999999996\n"
     ]
    }
   ],
   "source": [
    "# Examples of how axis affects the sum using NumPy :\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "layer_outputs = np.array([[4.8, 1.21, 2.835],\n",
    "                          [8.9, -1.81, 0.2],\n",
    "                          [1.41, 1.051, 0.026]])\n",
    "\n",
    "# print(layer_outputs)\n",
    "print('Sum without axis:')\n",
    "print(np.sum(layer_outputs))\n",
    "\n",
    "print('This will be identical to the above since default is None:')\n",
    "print(np.sum(layer_outputs, axis=None))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no axis specified we are just summing up all the values, even if they are varying in dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another way to think of it w/ a matrix == axis 0: columns:\n",
      "[15.11   0.451  3.061]\n"
     ]
    }
   ],
   "source": [
    "# axis 0\n",
    "\n",
    "print('Another way to think of it w/ a matrix == axis 0: columns:')\n",
    "print(np.sum(layer_outputs, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not what we want though, we want to sum the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But we want to sum the rows instead, like this with raw python:\n",
      "8.844999999999999\n",
      "7.29\n",
      "2.4869999999999997\n"
     ]
    }
   ],
   "source": [
    "print('But we want to sum the rows instead, like this with raw python:')\n",
    "\n",
    "for i in layer_outputs:\n",
    "    print(sum(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So we can sum axis 1 but note the current shape:\n",
      "[8.845 7.29  2.487]\n"
     ]
    }
   ],
   "source": [
    "# Using NumPy:\n",
    "\n",
    "print('So we can sum axis 1 but note the current shape:')\n",
    "print(np.sum(layer_outputs, axis=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to simplify the outputs to a single value per sample. For that reason we need a column vector with these values since it will let us normalize the whole batch of samples, sample-wise, with a single calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum axis 1, but keep the same dimensions as input:\n",
      "[[8.845]\n",
      " [7.29 ]\n",
      " [2.487]]\n"
     ]
    }
   ],
   "source": [
    "print('Sum axis 1, but keep the same dimensions as input:')\n",
    "print(np.sum(layer_outputs, axis=1, keepdims=True))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we keep the same dimensions as the input. Now, if we divide the array containing a batch of the outputs with this array, NumPy will preform this sample-wise. That means that it'll divide all the values from each output row by the corresponding row from the sum array. Since this sum in each row is a single value, it'll be used for the division with every value from the corresponding output row.\n",
    "\n",
    "Combining this all into a softmax class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = probabilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include the substraction of the largest input before we did the exponentiation to avoid exploding values. With softmax, thanks to normalization, we can substract any value from all of the inputs and it will not change the output....the relative proportions of the output probabilities do not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09003057 0.24472847 0.66524096]]\n"
     ]
    }
   ],
   "source": [
    "softmax = Activation_Softmax()\n",
    "\n",
    "softmax.forward([[1, 2, 3]])\n",
    "print(softmax.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09003057 0.24472847 0.66524096]]\n"
     ]
    }
   ],
   "source": [
    "softmax.forward([[-2, -1, 0]]) # substracted 3 --max from the list\n",
    "print(softmax.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18632372 0.30719589 0.50648039]]\n"
     ]
    }
   ],
   "source": [
    "# What if we divide the layer's output data by 2:\n",
    "\n",
    "#  [1, 2, 3] ---> [0.5, 1, 1.5]\n",
    "\n",
    "softmax.forward([[0.5, 1, 1.5]])\n",
    "print(softmax.output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output confidences have changed due to the nonlinearity of the exponentiation. This is why we need to scale all the input data to a neural network in the same way.\n",
    "\n",
    "Now we will add a new dense layer , setting it to have as many inputs as the previous layer has outputs and as many outputs as our data includes classes. Then we apply the softmax to this new layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create dense later with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
