{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Step Activation Function\n",
    "\n",
    "    if weights * inputs + bias >0 THEN neuron will fire and output a 1, otherwise it will output a  0\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linear Activation Function\n",
    "\n",
    "    Equation of a line y=x and the output value equals the input. \n",
    "    Usually applied to the last layer's output in the casae of a regression model(scaler value)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Sigmoid Activation Function\n",
    "\n",
    "    Better than step funtion due the need of optimizers. Sigmoid is more granular and informative. Used in Hidden Layers\n",
    "\n",
    "    y= 1/(1+e^-x)\n",
    "\n",
    "    y=0 and x=-inf,  y=0.5 at x=0,  y= 1 at x=inf\n",
    "\n",
    "    works better with NN due to range between 0 and 1, also keeps all information(Reversible)\n",
    "\n",
    "    Adds non linearity.\n",
    "\n",
    "    Sigmoid eventually gets replaced by Rectified Linear Units (ReLU)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Rectified Linear Activation Function\n",
    "\n",
    "    ReLU is close to being a linear one but remains non-linear due to the bend after 0\n",
    "\n",
    "    y=x when x>0\n",
    "    y=0 when x<0\n",
    "\n",
    "    Simple yet powerful activation funtion is the most widely used due to speed and efficiency."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Activation in the Hidden Layers\n",
    "\n",
    "        No matter what we do with the neurons' weights and biases, the outputs will always be perfectly linear to y=x. This linear nature will continue throughout the entire network\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Activation in a Pair of Neurons\n",
    "\n",
    "        We can see that by using ReLU activation, with different sets of weights(-/+) the slope can change and biases(left/right shift) cause horizontal movement. With the addition of another neuron, now we have 2...the bias of the second neuron shifted the function Vertically. With the 2nd nuerons weight to a -, we now have a neuron that has an activation and deactivation point, whne both neurons are active, they produce values in the range of the granulae, variable, and output. If any neuron is inactive, the pair will produce non-variable output.\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Activation in the Hidden Layers\n",
    "\n",
    "We start with a neural network with 2 hidden layers of 8 neurons each and the layers are NOT densly connected (each N from 1rst connects omly to one N in the 2nd). The model takes 1 value as an input and 1 value as output, just like the sin curve. The output layer uses liner activation, and the hidden layers use rectified linear actvation function. We will now hand tune the Network:\n",
    "        \n",
    "        -Starting with the first/top pair. setting all (3) weights to 1 and all (2) biases to zero, the slope of the overall funtion is simply y=x\n",
    "\n",
    "        - Changing W1 to 6, the slopehas become very steep and now matches the first part of the sin curve. But this function never ends because the neuron pair never deactivates \n",
    "\n",
    "        - By changing B2 to 0.7, this offsets the overall function Vertically upwards\n",
    "\n",
    "        - Setting W2 to a negative number -1, this causes a deactivation point to occur horizontally\n",
    "\n",
    "        - The previous ajustment causes the line that was previously aligned with the sin function to flip over the x axis. In order to flip this slope back. We can take the final weight connected to the output neuron W3 and simply flip it to a -1\n",
    "\n",
    "        - Now all we want is to offset this up vertically. In this Hand optimization we are going to use the first 7 pairs of neurons to create the sin wave's shape and the bottom pair to offset everything vertically.\n",
    "\n",
    "        - Setting the bias of the 2nd neuron in the bottom pair to 1 and the W3 to 0.7 we can vertically shift the line.\n",
    "\n",
    "        - Moving on to the next pair of neurons: The 2nd pair of neurons' activation is beginning too soon. We must ajust the funtion horizontally and then increase the slope. This is done by Adding a bias B1 to -0.42 and chaning the W1 to 3.5\n",
    "\n",
    "        - Now we use the same logic used before with the first pair to set the DEACTIVATION point. This is done by switching W2 to a -1 (negative value essentially flips the activation function) and adding a b2 of 0.27 (function is shifted vertically upwards. The positive bias means that the second neuron starts producing a significant output at a lower input value, and this effect is then reflected in the combined output of the neuron pair. )\n",
    "\n",
    "        - Now we can flip this section's function same as we did before by setting the weight to the output neuron from 1 to -1 \n",
    "\n",
    "        - And again , just like the first pair we use the botton pair to offset \n",
    "\n",
    "        -Finally  , for the third pair of neurons , we only begin the activation for the 3rd pair of hidden layer neurons when we wish for the sop to start going down......We ajust where we want these things to start by the horizontal shifts and flips that we do to the W2 , the W2 turning into a negative creates the flip/deactivation point 'the funtion has become a quesiton of when this neuron deactivates\n",
    "\n",
    "\n",
    "Now we keep applying the same logic to the rest of the neuron pairs as we go trhough mapping the sin function. After passing data through the Network do we see how the neuron's areas of effect come into play--- only when bith neurons are activated. When we input 0.08 we see that its the top layer(the very begining of the sin function/close to zero which makes sense)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Activation Function Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "\n",
    "output = []\n",
    "for i in inputs:\n",
    "    if i > 0:\n",
    "        output.append(i)\n",
    "    else:\n",
    "        output.append(0)\n",
    "        \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "# We can write the code more simply as we just need to take the larger of the 2 values: 0 or the neuron value\n",
    "\n",
    "\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "\n",
    "output = []\n",
    "\n",
    "for i in inputs:\n",
    "    output.append(max(0, i))\n",
    "    \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "# NumPy contains an equivalent -  np.maximum()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "\n",
    "output = np.maximum(0, inputs)\n",
    "print(output)\n",
    "\n",
    "# this method compares each element of the input list(or an array) and returns an object of the same shape filled with new values.\n",
    "# This will be used in our new rectified linear activation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data\n",
    "import numpy as np\n",
    "import nnfs\n",
    "\n",
    "\n",
    "# The init() function sets the random seed of numpy to a constant, and sets the default data type of numpy to a float type \n",
    "# that is more suitable for neural networks and similar works.\n",
    "nnfs.init()\n",
    "\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        # notice the size of the matrix is n_inputs X n_neurons and not the other way around, we do this to avoid having to transpose the \n",
    "        # weights matrix later. \n",
    "        \n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # 1 X n_n_neurons  Array\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # calculate output values from input\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.        ]\n",
      " [0.         0.00011395 0.        ]\n",
      " [0.         0.00031729 0.        ]\n",
      " [0.         0.00052666 0.        ]\n",
      " [0.         0.00071401 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#  Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# This function generates a 2D dataset with a spiral distribution. It returns the feature vectors (X) and the corresponding labels (y).\n",
    "# The samples parameter specifies the number of data points per class (so you'll have 300 points in total, because you have 3 classes), \n",
    "\n",
    "# Create Dense layera with 2 input features and 3 output values\n",
    "dense1 =  Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Forward pass through activation function\n",
    "# Takes in output from previous layer\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Let's see output of the first few samples\n",
    "print(activation1.output[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative values have been clipped(modified to be zero) from the previous layers outputs(checkout the end of NNFS_3 to see outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
