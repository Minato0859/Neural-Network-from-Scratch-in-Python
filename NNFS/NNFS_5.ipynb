{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Network Error with Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Cross-Entropy Loss"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Most commonly used loss funtion with the softmax activation on the output layer\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "Suppose we have a softmax output :\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "\n",
    "and the desired prediction is the first class, then the desired probability distribution is:\n",
    "[1, 0, 0]\n",
    "\n",
    "Arrays and vectors like this are calles one-hot and when comparing a models results to one-hot vectors , other parts of the equation zero out making the calculation very simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# An example output from and the output layet of the neural network\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "# Ground truth\n",
    "target_output = [1, 0, 0]\n",
    "\n",
    "loss =  -(math.log(softmax_output[0])*target_output[0] +\n",
    "         math.log(softmax_output[1])*target_output[1] +\n",
    "         math.log(softmax_output[2])*target_output[2])\n",
    "\n",
    "print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "#same as\n",
    "\n",
    "loss = -(math.log(softmax_output[0]))\n",
    "\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1 = [0.22, 0.6, 0.18]\n",
    "case 2 = [0.32, 0.36, 0.32]\n",
    "\n",
    "in both cases, the argmax of these vectors will return teh second class as the prediction but the models confidence\n",
    "level isnt the same.\n",
    "\n",
    "The Categorical Cross-Entropy Loss accounts for that and outouts a larger loss the lower the confindence is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "-0.05129329438755058\n",
      "-0.10536051565782628\n",
      "-0.2231435513142097\n",
      "...\n",
      "-1.6094379124341003\n",
      "-2.3025850929940455\n",
      "-2.995732273553991\n",
      "-4.605170185988091\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(math.log(1.))\n",
    "print(math.log(0.95))\n",
    "print(math.log(0.9))\n",
    "print(math.log(0.8))\n",
    "print('...')\n",
    "print(math.log(0.2))\n",
    "print(math.log(0.1))\n",
    "print(math.log(0.05))\n",
    "print(math.log(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.5\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "# We need a way to dynamcally calculate cross-entropy\n",
    "\n",
    "softmax_outputs = [[0.7, 0.1, 0.2],\n",
    "                   [0.1, 0.5, 0.4],\n",
    "                   [0.02, 0.9, 0.08]]\n",
    "\n",
    "class_targets = [0, 1, 1]\n",
    "\n",
    "for targ_idx, distribution in zip(class_targets, softmax_outputs): #zip() function lets us iterate over multiple iterables at the same time\n",
    "    print(distribution[targ_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "# With NumPy , we create a NumPy array this time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                   [0.1, 0.5, 0.4],\n",
    "                   [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = [0, 1, 1]\n",
    "\n",
    "print(softmax_outputs[[0, 1, 2], class_targets])\n",
    "# Here we index an array in NumPy, but we could also use a range() instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "print(softmax_outputs[range(len(softmax_outputs)), class_targets]) #This returns the list of confidences at the target indices for \n",
    "# each of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n"
     ]
    }
   ],
   "source": [
    "# with the negative log\n",
    "print(-np.log(softmax_outputs[\n",
    "    range(len(softmax_outputs)), class_targets]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we want is the average loss per batch. The most basic way to calculate avg in python is the arithmetic mean: sum(iterable)/len(iterable). However NumPy has a method that computes this average on arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "neg_log = -np.log(softmax_outputs[\n",
    "    range(len(softmax_outputs)), class_targets\n",
    "])\n",
    "\n",
    "average_loss = np.mean(neg_log)\n",
    "\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "targets can either be one-hot coded or they can be sparse, which means that the numbers they contain are the correct class numbers. We want the loss funtion to be able to accept any of these forms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "class_targets = np.array([[1, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [0, 1, 0]])\n",
    "\n",
    "# Probabilities for target values\n",
    "# Only if categorical labels\n",
    "\n",
    "if len(class_targets.shape) == 1:\n",
    "    correct_confidences = softmax_outputs[range(len(softmax_outputs)), class_targets]\n",
    "# Mask values - only for one-hot encoded labels\n",
    "elif len(class_targets.shape) == 2:\n",
    "    correct_confidences = np.sum(\n",
    "        softmax_outputs*class_targets, \n",
    "        axis=1)\n",
    "    \n",
    "# Losses\n",
    "neg_log = -np.log(correct_confidences)\n",
    "\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bilal\\AppData\\Local\\Temp\\ipykernel_19248\\3370398627.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  print(-np.log(0))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(-np.log(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.e**(-np.inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bilal\\AppData\\Local\\Temp\\ipykernel_19248\\3055906385.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  print(np.mean([1, 2, 3, -np.log(0)]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.mean([1, 2, 3, -np.log(0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.11809565095832\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.999999505838704e-08\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(1+1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000494736474e-07\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(1-1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use these small ajustments of 1+/- 1e-7 in order to prevenet loss from being exactly 0 and avoid overshifting confidence to 1\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# method to preform clipping on an array\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[39m# i.e The clip() function in numpy serves to ensure that all elements in an array are within a specified range.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# It \"clips\" any values that fall outside that range to the boundary values.\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m y_pred_clipped \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(y_pred, \u001b[39m1e-7\u001b[39m, \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1e-7\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# method to preform clipping on an array\n",
    "\n",
    "# i.e The clip() function in numpy serves to ensure that all elements in an array are within a specified range.\n",
    "# It \"clips\" any values that fall outside that range to the boundary values.\n",
    "\n",
    "y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Categorical Cross-Entropy Loss Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later we will discuss more loss functions but regardless of the funtion, the overall loss is always a mean value of all sample losseS. For now we will creat a Loss Class containing the Calculate method that will call the loss object's forward methd and calculate the mean value of the returned sample losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Loss class\n",
    "class Loss:\n",
    "    \n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        \n",
    "        # calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "    \n",
    "        # calculate mean loss\n",
    "        data_loss = np.mean(sample_losses) #array of -log values of correct_confidences, either form 1_D or 2_D\n",
    "        \n",
    "        # return loss\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert our loss code into a class for convenience down the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss): #inherit the loss class\n",
    "    \n",
    "    #  Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        # Number of samples in batch\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Probabilities for target values\n",
    "        # only if categorical variables\n",
    "        if len(y_true.shape) == 1:\n",
    "            # for each sample (row), it's pulling out the predicted probability of the true class label. y_true is probably a 1_D array containing the true class label in each entry associated with each row\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true] \n",
    "        \n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1) #matrix multiplication to end up with a 1_d NumPy array, \n",
    "            # summed across the columns, so for every row you're summing all the multiplies values\n",
    "            \n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences) #take the negative log as per the cross-entropy formula\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[0.7 0.5 0.9]\n",
      "[[0.7  0.1  0.2 ]\n",
      " [0.1  0.5  0.4 ]\n",
      " [0.02 0.9  0.08]]\n",
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(len(class_targets.shape))\n",
    "print(correct_confidences)\n",
    "print(softmax_outputs)\n",
    "print(class_targets)\n",
    "# print(negative_log_likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate(softmax_outputs, class_targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Everything up to this Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data\n",
    "import numpy as np\n",
    "import nnfs\n",
    "\n",
    "# The init() function sets the random seed of numpy to a constant, and sets the default data type of numpy to a float type \n",
    "# that is more suitable for neural networks and similar works.\n",
    "nnfs.init()\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        # notice the size of the matrix is n_inputs X n_neurons and not the other way around, we do this to avoid having to transpose the \n",
    "        # weights matrix later. \n",
    "        \n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # 1 X n_n_neurons  Array\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # calculate output values from input\n",
    "        self.output = np.maximum(0, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Loss class\n",
    "class Loss:\n",
    "    \n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        \n",
    "        # calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "    \n",
    "        # calculate mean loss\n",
    "        data_loss = np.mean(sample_losses) #array of -log values of correct_confidences, either form 1_D or 2_D\n",
    "        \n",
    "        # return loss\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "loss: 1.0986104011535645\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create dense later with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take the output of the previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "\n",
    "# Make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Make forward pass through activation function\n",
    "# it takes the output of the first dense layer here\n",
    "activation1.forward(dense1.output) #ReLU activation which considers max(0, input ) and the input is w*X +bias\n",
    "\n",
    "# Make a forward pass through second Dense layer\n",
    "# it takes outputs of the activation function which took the first layer's outputs as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Make a forward pass through 2nd activation function\n",
    "# it takes the outputs of the second dense layer here\n",
    "activation2.forward(dense2.output) #Softmax activation which exponentiates , substracts with max, then divides each Exp batch by sum of each row\n",
    "\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])\n",
    "\n",
    "\n",
    "# Preforms a forward pass through activation function\n",
    "# it takes the output of second dense layer here and returns loss\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "# Print loss value\n",
    "print(f'loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is a useful metric for optimisation of a model, but it is also used along with accuracy which describes how often the largest confidence is the correct class in terms of a fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Probabilities of 3 samples\n",
    "softmax_outputs = np.array([[0.7, 0.2, 0.1],\n",
    "                            [0.5, 0.1, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "# Target (ground-truth) labels for 3 samples\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "# Calculate values along second axis (axis of index 1)\n",
    "predictions = np.argmax(softmax_outputs, axis=1)\n",
    "# If targets are onehot coded - convert them\n",
    "if len(class_targets.shape) == 2:\n",
    "    class_targets = np.argmax(class_targets, axis=1)\n",
    "# True evaluates to 1; False to 0\n",
    "accuracy = np.mean(predictions == class_targets)\n",
    "\n",
    "print(f'accuracy = {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "loss: 1.0986104011535645\n"
     ]
    }
   ],
   "source": [
    "# full code + accuracy \n",
    "\n",
    "from nnfs.datasets import spiral_data\n",
    "import numpy as np\n",
    "import nnfs\n",
    "\n",
    "\n",
    "# The init() function sets the random seed of numpy to a constant, and sets the default data type of numpy to a float type \n",
    "# that is more suitable for neural networks and similar works.\n",
    "nnfs.init()\n",
    "\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        # notice the size of the matrix is n_inputs X n_neurons and not the other way around, we do this to avoid having to transpose the \n",
    "        # weights matrix later. \n",
    "        \n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # 1 X n_n_neurons  Array\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # ReLU activation\n",
    "class Activation_ReLU:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # calculate output values from input\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Softmax activation\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = probabilities\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Common Loss class\n",
    "class Loss:\n",
    "    \n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        \n",
    "        # calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "    \n",
    "        # calculate mean loss\n",
    "        data_loss = np.mean(sample_losses) #array of -log values of correct_confidences, either form 1_D or 2_D\n",
    "        \n",
    "        # return loss\n",
    "        return data_loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create dense later with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take the output of the previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "\n",
    "# Make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Make forward pass through activation function\n",
    "# it takes the output of the first dense layer here\n",
    "activation1.forward(dense1.output) #ReLU activation which considers max(0, input ) and the input is w*X +bias\n",
    "\n",
    "# Make a forward pass through second Dense layer\n",
    "# it takes outputs of the activation function which took the first layer's outputs as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Make a forward pass through 2nd activation function\n",
    "# it takes the outputs of the second dense layer here\n",
    "activation2.forward(dense2.output) #Softmax activation which exponentiates , substracts with max, then divides each Exp batch by sum of each row\n",
    "\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])\n",
    "\n",
    "\n",
    "# Preforms a forward pass through activation function\n",
    "# it takes the output of second dense layer here and returns loss\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "# Print loss value\n",
    "print(f'loss: {loss}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.34\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "# calculate accuracy of model from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "\n",
    "print('acc:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
